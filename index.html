<!DOCTYPE html>
<html>
<body>
<h1>Machine Learning: Final Project</h1>
<h2>Cleaning the Data and Building the Model</h2>
<p>I began by exploring the data, specifically looking for large segments of NA values.
I chose to remove columns that had over 19200 NA values, and then applied near-zero variance,
which led me to remove the 'new_window' variable.  I also examined the variable names and removed
several variables that did not seem like they would have significant impact on the outcome, 
such as 'user_name', several time stamp variables, and the 'X' variable, which appeared to be an
observation number.</p>
</br>
<p>I used the caret package to build an initial random forest model used 53 features to predict the classe.  I
applied the variable importance function and found that there was a drop off in importance after the top nine features (see Figure 1).  I then built a second random forest model using the top 9 features, which is the version I used for submission due to time considerations.  After applying variable importance to the second model, I found that I could likely remove the 'accel_dumbbell_y' variable and possibly the 'roll_forearm' as they received very
low overall scores.</p>
</br>
<h3>Figure 1: Model 1 Variable Importance</h3>
<table border="1" style="width:100%">
 <tr>
   <td>Rank</td>
   <td>Feature Name</td>
   <td>Importance</td>
 </tr>
 <tr>
   <td>1</td>
   <td>num_window</td>
   <td>100.00</td>
 </tr>
 <tr>
   <td>2</td>
   <td>roll_belt</td>
   <td>64.681</td>
 </tr>
 <tr>
   <td>3</td>
   <td>pitch_forearm</td>
   <td>39.350</td>
 </tr>
 <tr>
   <td>4</td>
   <td>yaw_belt</td>
   <td>33.833</td>
 </tr>
 <tr>
   <td>5</td>
   <td>magnet_dumbbell_z</td>
   <td>29.867</td>
 </tr>
 <tr>
   <td>6</td>
   <td>magnet_dumbbell_y</td>
   <td>29.476</td>
 </tr>
 <tr>
   <td>7</td>
   <td>pitch_belt</td>
   <td>28.868</td>
 </tr>
 <tr>
   <td>8</td>
   <td>roll_forearm</td>
   <td>23.137</td>
 </tr>
 <tr>
   <td>9</td>
   <td>accel_dumbbell_y</td>
   <td>14.438</td>
 </tr>
 <tr>
   <td>10</td>
   <td>magnet_dumbbell_x</td>
   <td>11.443</td>
 </tr>
 <tr>
   <<td>11</td>
   <td>accel_forearm_x</td>
   <td>11.354</td>
 </tr>
 <tr>
   <td>12</td>
   <td>roll_dumbbell</td>
   <td>10.444</td>
 </tr>
 <tr>
   <td>13</td>
   <td>accel_belt_z</td>
   <td>10.227</td>
 </tr>
 <tr>
   <td>14 - 20</td> 
   <td>seven other features</td>
   <td>under 9.450</td>
 </tr>
</table>              
<h2>Cross-Validation</h2>
<p>After eliminating the NA rows, near zero rows, and other rows of assessed limited value, I used the createDataPartition function to create a training set and test set.  To build the initial model I used 2-Fold cross validation for speed, since the goal of this model was to eliminate less important features.  For the second model I used 10-Fold cross validation. I chose 10 folds to balance bias and variance-- smaller ks would have more bias towards the training data, but less variance.  Larger ks would have less bias, but more variance.</p>
<h2>Expected Out of Sample Error</h2>
<p>Because I used k-fold cross validation to pick predictors, I estimated my out of sample error on the independent test set.  I used a confusion matrix on the predicted results vs. actual results.  Accuracy was 100%.</p>
<h2>R Code</h2>
<p>setwd('/Users/marqueze/machinelearning')</p>
<p>library(caret)</p>
<p>lift<-read.csv('pml-training.csv', na.strings=c('NA', '#DIV/0!'))</p>
<p>naval<-which(as.numeric(colSums(is.na(lift)))>19200) #100 columns</p>
<p>newlift<- subset(lift, select = -(naval))</p>
<p>nzv<-nearZeroVar(newlift, saveMetrics=TRUE)</p>
<p>newlift<- subset(newlift, select = -(c(new_window, raw_timestamp_part_1, raw_timestamp_part_2,</p>
<p>                                    cvtd_timestamp, X, user_name)))</p>
<p>inTrain <- createDataPartition(newlift$classe, p = .75, list=FALSE)</p>
<p>training<-newlift[inTrain,]</p>
<p>test<-newlift[-inTrain,]</p>
<p></p>
<p>set.seed(1555)</p>
<p>crtl<-trainControl(method="cv", number=2)</p>
<p>modFit <- train(classe~ .,data=training, method="rf", prox=TRUE, trControl=crtl)</p>
<p>varImp(modFit)</p>
<p>crtl<-trainControl(method="cv", number=10)</p>
<p>modFit2<-train(classe ~ num_window + roll_belt + pitch_forearm + yaw_belt + </p>
<p>              magnet_dumbbell_z + magnet_dumbbell_y + pitch_belt + roll_forearm+</p>
<p>              accel_dumbbell_y, data=training, method="rf", prox=TRUE,</p>
<p>              trControl=crtl)</p>
<p>confusionMatrix(test$classe, predict(modFit2, test))</p>
</br>
<p>March 21, 2015</p>
</body>
</html>
